## PROJECT ASSIGNMENT 1
### TEK5020/9020 â€“ PATTERN RECOGNITION
Autumn 2024

#### 1 Introduction
This assignment is intended to illustrate the process in a hypothetical pattern recognition application. In short, it involves finding the best feature combination for a given dataset, constructing various classifiers, and evaluating the results. For simplicity, we will consider problems with only two classes.

To solve the assignment, you need to write software that reads the data, splits the dataset into training and test sets, trains a classifier of the desired type on a specified feature combination, tests it by classifying the objects in the test set, and estimates error rates. You are free to use any available programming tools (e.g., Matlab or Python).

A concise report (maximum approximately 10 pages) should be written, describing what has been done and what results have been achieved. For each dataset, the following should be provided:

1. **Ranking of feature combinations**, based on the nearest neighbor rule.
2. **Ranking of the classifiers** (three classifier types) for the best feature combinations.

In both cases, the evaluation metrics should be provided, i.e., estimated error rates for each of the tested feature combinations and each of the classifier types used. Additionally, you should briefly answer some questions. Feel free to include code listings, but this is not a requirement.

The assignment can be carried out as group work, where 2-3 students submit a joint report.

#### 2 Datasets

The datasets to be used are stored as text files, where each line corresponds to one object. The first element in each line indicates the class membership (the value 1 or 2), and the other elements in the line specify the feature values associated with the same object.

Three datasets shall be used in the assignment (`Dataset_1`, `Dataset_2`, and `Dataset_3`). The data files are attached to the assignment. The first two sets are synthetic, i.e., they are generated by sampling from known probability density functions. The last one is generated by extracting shape features from segments (silhouettes) of two different car models. Dataset 1 contains 300 objects with 4 features, dataset 2 consists of 300 objects with 3 features, and dataset 3 has 400 objects with 4 features.

The classifiers in this assignment should be trained and tested on different selections of objects, i.e., each dataset must be split into a training set and a test set. To make it easy to compare results, it is important that all students use the same partitioning. Each of the datasets should therefore be divided such that the training set consists of odd-numbered objects (i.e., objects 1, 3, 5, 7, etc.) while the remaining objects (no. 2, 4, 6, 8, etc.) are placed in the test set.

#### 3 Training of Classifiers
In this assignment, the following classifiers should be used:

1. Minimum error rate classifier with normal distribution assumption,
2. Least squares method,
3. Nearest neighbor classifier.

A brief overview of these classifiers is provided below.

##### 3.1 Minimum Error Rate Classifier
This classifier assigns an object with feature vector $\vec{x}$ to class $\omega_k$ if

$$
P(\omega_k|\vec{x}) = \max_{i} P(\omega_i|\vec{x}), \quad i = 1, \dotsc, c
$$

where $c$ is the number of classes. Assuming normally distributed class-conditional density functions, we obtain the discriminant functions

$$
g_i(\vec{x}) = \vec{x}^T W_i \vec{x} + \vec{w}_i^T \vec{x} + w_{i0}
$$

where

$$
W_i = -\frac{1}{2} \Sigma_i^{-1} \tag{1}
$$

$$
\vec{w}_i = \Sigma_i^{-1} \vec{\mu}_i \tag{2}
$$

and

$$
w_{i0} = -\frac{1}{2} \vec{\mu}_i^T \Sigma_i^{-1} \vec{\mu}_i - \frac{1}{2} \ln |\Sigma_i| + \ln P(\omega_i) \tag{3}
$$

Here, $\Sigma_i$ and $\vec{\mu}_i$ are the covariance matrix and the mean vector of class $\omega_i$, respectively. These quantities are usually unknown and must be estimated from the training data. Here, the maximum likelihood method should be used to estimate these quantities. This means that the mean vector and covariance matrix of class $\omega_i$ are estimated as

$$
\hat{\vec{\mu}}_i = \frac{1}{n_i} \sum_{j=1}^{n_i} \vec{x}_{ij}
$$

and

$$
\hat{\Sigma}_i = \frac{1}{n_i} \sum_{j=1}^{n_i} (\vec{x}_{j} - \hat{\vec{\mu}}_i)(\vec{x}_{j} - \hat{\vec{\mu}}_i)^T
$$

where $n_i$ is the number of objects in the training set for class $\omega_i$ and $\vec{x}_{j}$ is the feature vector of object $j$ in class $\omega_i$.

Because we are only considering two-class problems here, we can use

$$
g(\vec{x}) = g_1(\vec{x}) - g_2(\vec{x})
$$

as the discriminant function. With this, $\vec{x}$ is assigned to class $\omega_1$ if $g(\vec{x}) > 0$ and to class $\omega_2$ otherwise.

As part of the project assignment, you should create a function that trains a minimum error rate classifier as outlined above. You need to create a function that, based on the training objects, calculates the matrices $W_i$, vectors $\vec{w}_i$, and constants $w_{i0}$ as given in equations (1), (2), and (3) for $i = 1, 2$. Since the prior probabilities $P(\omega_i)$ included in equation (3) are unknown here, they must be estimated based on the number of training objects from each class in the dataset.

The function should be used to construct classifiers for selected feature combinations from all three datasets.

##### 3.2 Least Squares Method
This method yields a linear classifier with a discriminant function given by

$$
g(\vec{x}) = \vec{a}^T \vec{y} \tag{4}
$$

where $\vec{a} = [a_0, a_1, a_2, \dotsc, a_n]^T$ is the extended vector of weights and $\vec{y} = [1, x_1, x_2, \dotsc, x_n]^T$ is the extended vector of features. Let $\vec{Y}$ be a matrix of dimension $n \times (d+1)$, which contains all the extended training vectors stored row-wise, i.e.,

$$
\vec{Y} = \begin{bmatrix}
1 & x_{11} & x_{12} & \dotsb & x_{1n} \\
1 & x_{21} & x_{22} & \dotsb & x_{2n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \dotsb & x_{nn}
\end{bmatrix}
$$

Further, we define an $n$-dimensional vector $\vec{b} = [b_1, b_2, \dotsc, b_n]^T$ where each element $b_i$ is given as $b_i = 1$ for $\vec{y}_i$ in class $\omega_1$ and $b_i = -1$ otherwise.

The vector $\vec{a}$ is now chosen such that the length of the error vector $\vec{e} = \vec{Y} \vec{a} - \vec{b}$ is minimized. That is, the cost function:

$$
J(\vec{a}) = \vec{e}^T \vec{e} = (\vec{Y} \vec{a} - \vec{b})^T (\vec{Y} \vec{a} - \vec{b})
$$

is minimized. This can be done by solving the normal equation

$$
\vec{Y}^T \vec{Y} \vec{a} = \vec{Y}^T \vec{b}
$$

In other words, you should write a function that, based on the **training objects**, calculates the vector $\vec{a}$ for an arbitrary combination of features, so that the classifier is then given by the discriminant function $g(\vec{x}) = \vec{a}^T \vec{y}$ (equation 4).

##### 3.3 Nearest Neighbor Classifier
This is a computationally simple classifier that simultaneously provides good and reliable classifications. For each object $\vec{x}$ to be classified, the distance to all training objects in the training set is calculated, and $\vec{x}$ is assigned the same class as the nearest training object $\vec{x}_k$, where the distance is given by

$$
\|\vec{x} - \vec{x}_k\| = \min_{i} \|\vec{x} - \vec{x}_i\|
$$

#### 4 Evaluation 

Evaluation of a classifier should be performed by classifying the objects in the test set and comparing the classification result with the given class membership for each object.

As an error rate estimate, the ratio between the number of misclassified objects and the total number of objects in the test set is usually used:

$$
\hat{P}(e) = \frac{n_{\text{errors}}}{n_{\text{total}}}
$$

This is the error estimate that should be reported for each of the tested feature combinations.

#### 5 Execution of the Assignment

For each of the three datasets, you should:

1. Use the nearest neighbor classifier to estimate the error rate for all combinations of features of a given dimension (systematic testing). This should be done for all possible dimensions (d = 1, 2, ..., number of features).
2. For the best combination within each possible feature dimension, find the best classifier among the three that are implemented.

#### 6 Concluding Questions
Finally, you should briefly answer the following questions:

1. Why is it reasonable to use the nearest neighbor classifier to find favorable feature combinations?
2. Why might it be reasonable in a practical application to find a linear or quadratic classifier to replace the nearest neighbor classifier?
3. Why is it unfavorable to use the same dataset both for training and evaluation of a classifier?
4. Why does a linear classifier give poor results for dataset 2?
